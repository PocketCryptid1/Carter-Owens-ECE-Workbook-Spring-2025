# Lecture 3

## Pipelined processors

* Prefetching 150 instructions
* lots of branch instructions
* if a single branch in incorrectly predicted, then all the prior instructions are useless (Wrong-Path instructions)

## Labs

if you have your own linux machine the suite should be easy to compile

## Memory

* early memory was read only
* before transistors we had diode arrays and mechanical systems
* moved to core style memory which we currently use
* now we use semiconductors which can be tiny (See VLSI classes)
* dram has destructive reads and bleeds charge

### DRAM

* bit line(x axis)
* word line(y axis)
* reads entire row then picks single value after
* Packaged in DIMMs
* Three stages of read/write
  * Row access
  * Column access
  * precharge
  
## CPU - memory bottleneck

* high speed performance is usually limited by memory bandwidth and latency
* CPUs have higher performance than ram and that gap has been increasing
* physical size effects latency
* fix issues with caching
* uses SRAM(much faster but more expensive)

## Typical memory reference patterns

* instruction fetches have spacial and temporal locality
* addresses are reused often (loops)
* Stack accesses have temporal locality
* we have lots of references to the same place
* array accesses have some locality but a lot less so
  
## Caches

* caches exploit both temporal and spacial locality
* exploit spacial locality by pulling neighboring data into cache after a read
* exploit temporal locality by keeping data in cache to be reused

### Advanced Cache optimizations

* Reduce hit time
  * keep cache smaller
    * smaller memory indexes faster
    * Ex: L1 caches have remained the same size for 3 generations of AMD processors
    * main variable in hit time
  * simple => Direct Mapping
  * way prediction
    * how to combine fast hit time of Direct map and have lower misses of 2 way cache
    * dont pull both ways down, only pull one that is predicted
      * if way misses then check the other
      * makes pipeline timing hard because hit time is variable
  * trace cache
    * pack multiple non contiguous blocks int one contiguous trace cache line
      * intel processor uses micro-ops
        * each instruction contains multiple micro-ops
        * most of the pipeline only sees micro-ops
        * operates more like a RISC processor despite being CISC
      * processor learns "Hot-Path"
        * most commonly taken branch path
        * the fetch portion learns hot path and creates a separate cache
        * special cache holds hot path micro-ops in trace-cache
        * pastes all the branches together into single line
      * avoids actual branch prediction but gets many of the benefits
      * issues
        * how to update path
        * Example
          * first million instructions path 1 is hot
          * after those instructions 1 stops appearing and 2 becomes hot
          * after some time 2 cools down and 3 heats up
          * there are duplicate entries in the paths
        * changing path is a complex process
        * mispredictions have a long recovery
        * these issues led to the pentium 4 completely abandoning trace caches
    * increasing cache bandwidth by pipelining
    * increasing cache bandwidth with non-blocking caches
      * allows data cache to supply hits during a miss recovery
      * "Hit under Miss" keep serving CPU requests while handling a miss
      * "miss under miss" successive misses reduce average miss penalty by overlapping misses
        * significantly increases complexity because there may be multiple outstanding memory accesses
        * Pentium Pro allows 4 outstanding misses
      * only really effective in data cache because things are out of order unlike instruction cache

## After Class ToDo

* add paper reading to trello